{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "podcast_to_headline_del_spon_sugar2_complete.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flora0110/podcast_segment_headine/blob/main/podcast_to_headline_del_spon_sugar2_complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Textsplit"
      ],
      "metadata": {
        "id": "gKH2OWuXr3Oc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtpEpr1erfko",
        "outputId": "02075549-80d3-4a3c-f7a3-b65feea6195c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting word2vec\n",
            "  Downloading word2vec-0.11.1.tar.gz (42 kB)\n",
            "\u001b[?25l\r\u001b[K     |███████▊                        | 10 kB 27.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 20 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 30 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 40 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 42 kB 402 kB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from word2vec) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.7/dist-packages (from word2vec) (1.21.6)\n",
            "Building wheels for collected packages: word2vec\n"
          ]
        }
      ],
      "source": [
        "!pip install word2vec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from gensim.models import word2vec\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "IhACuETwsAZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download toy corpus for wordvector training and example text"
      ],
      "metadata": {
        "id": "-fDUTHDJsOFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_path = './text8'  # be sure your corpus is cleaned from punctuation and lowercased\n",
        "if not os.path.exists(corpus_path):\n",
        "    !wget http://mattmahoney.net/dc/text8.zip\n",
        "    !unzip {corpus_path}"
      ],
      "metadata": {
        "id": "yW89tWVysQOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "links = {'podcast_text': 'https://raw.githubusercontent.com/grace-boop/podcast/main/flora_sugarl'}  # siddartha\n",
        "\n",
        "for link in links.values():\n",
        "    text_path = os.path.basename(link)\n",
        "    if not os.path.exists(text_path):\n",
        "        !wget {link}"
      ],
      "metadata": {
        "id": "ZuUN-5ITzk8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train wordvectors"
      ],
      "metadata": {
        "id": "yeEijQ0ttOvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "wrdvec_path = 'wrdvecs.bin'\n",
        "if not os.path.exists(wrdvec_path):\n",
        "  sentences = word2vec.Text8Corpus('./text8')\n",
        "  model = word2vec.Word2Vec(sentences, cbow_mean=1, iter=5, hs=1, sample=0.00001, window=15, size=200)"
      ],
      "metadata": {
        "id": "W18gaxqMtRfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(wrdvec_path)"
      ],
      "metadata": {
        "id": "kllPGhebtdv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = word2vec.Word2Vec.load(wrdvec_path)\n",
        "wrdvecs = pd.DataFrame(model.wv.vectors, index=model.wv.vocab)"
      ],
      "metadata": {
        "id": "azdJN5OUtg4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get sentence tokenizer"
      ],
      "metadata": {
        "id": "8MZoiGawtn5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textsplit"
      ],
      "metadata": {
        "id": "1LQZtqFjtp-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textsplit.tools import SimpleSentenceTokenizer\n",
        "sentence_tokenizer = SimpleSentenceTokenizer()"
      ],
      "metadata": {
        "id": "nCqbCLb4tre6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## run get_penalty and split_optimal"
      ],
      "metadata": {
        "id": "TwRHiwHlt7Oy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from textsplit.tools import get_penalty, get_segments\n",
        "from textsplit.algorithm import split_optimal, split_greedy, get_total"
      ],
      "metadata": {
        "id": "RNI8RZOGt68T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### podcast"
      ],
      "metadata": {
        "id": "t-EySFDdwZ3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title segment target length in sentences { display-mode: \"form\" }\n",
        "segment_len =  25#@param {type:\"integer\"}"
      ],
      "metadata": {
        "id": "paSWEfbGv0-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "link = links['podcast_text']\n",
        "book_path = os.path.basename(link)\n",
        "\n",
        "with open(book_path, 'rt') as f:\n",
        "    text = f.read()  #.replace('\\n', ' ')  # punkt tokenizer handles newlines not so nice\n",
        "\n",
        "sentenced_text = sentence_tokenizer(text)\n",
        "strs = \" \"\n",
        "for i in range(len(sentenced_text)):\n",
        "  if(sentenced_text[i] != \" \"):\n",
        "    strs =sentenced_text[i]\n",
        "  if(i+1<len(sentenced_text)):\n",
        "    if(strs == sentenced_text[i+1]):\n",
        "      sentenced_text[i+1]=\" \"\n",
        "vecr = CountVectorizer(vocabulary=wrdvecs.index)\n",
        "\n",
        "sentence_vectors = vecr.transform(sentenced_text).dot(wrdvecs)\n",
        "new_length = len(sentenced_text)**0.6\n",
        "print(new_length)\n",
        "penalty = get_penalty([sentence_vectors], new_length)\n",
        "#penalty = get_penalty([sentence_vectors], segment_len)\n",
        "print('penalty %4.2f' % penalty)\n",
        "\n",
        "optimal_segmentation = split_optimal(sentence_vectors, penalty, seg_limit=250)\n",
        "segmented_text = get_segments(sentenced_text, optimal_segmentation)\n",
        "\n",
        "print('%d sentences, %d segments, avg %4.2f sentences per segment' % (\n",
        "    len(sentenced_text), len(segmented_text), len(sentenced_text) / len(segmented_text)))\n",
        "\n",
        "with open(book_path + '.seg', 'wt') as f:\n",
        "    for i, segment_sentences in enumerate(segmented_text):\n",
        "        segment_str = ' // '.join(segment_sentences)\n",
        "        gain = optimal_segmentation.gains[i] if i < len(segmented_text) - 1 else 0\n",
        "        segment_info = ' [%d sentences, %4.3f] ' % (len(segment_sentences), gain) \n",
        "        print(segment_str + '\\n8<' + '=' * 30 + segment_info + \"=\" * 30, file=f)\n",
        "\n",
        "greedy_segmentation = split_greedy(sentence_vectors, max_splits=len(optimal_segmentation.splits))\n",
        "greedy_segmented_text = get_segments(sentenced_text, greedy_segmentation)\n",
        "lengths_optimal = [len(segment) for segment in segmented_text for sentence in segment]\n",
        "lengths_greedy = [len(segment) for segment in greedy_segmented_text for sentence in segment]\n",
        "df = pd.DataFrame({'greedy':lengths_greedy, 'optimal': lengths_optimal})\n",
        "df.plot.line(figsize=(18, 3), title='Segment lenghts over text')\n",
        "df.plot.hist(bins=30, alpha=0.5, figsize=(10, 3), title='Histogram of segment lengths')\n",
        "\n",
        "totals = [get_total(sentence_vectors, seg.splits, penalty) \n",
        "          for seg in [optimal_segmentation, greedy_segmentation]]\n",
        "print('optimal score %4.2f, greedy score %4.2f' % tuple(totals))\n",
        "print(totals)\n",
        "print(tuple(totals))\n",
        "print('ratio of scores %5.4f' % (totals[0] / totals[1]))"
      ],
      "metadata": {
        "id": "0saLwQ6huDLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 顯示分段"
      ],
      "metadata": {
        "id": "zqp55X5LyUdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(segmented_text)):\n",
        "  print(segmented_text[i])\n",
        "  "
      ],
      "metadata": {
        "id": "Rsb59khayAck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_sentence = []\n",
        "for i in range(len(segmented_text)):\n",
        "  first_sentence.append(segmented_text[i][0].strip().rstrip().rstrip('.').rstrip('?'))\n",
        "  print(segmented_text[i][0])\n",
        "  print(type(segmented_text[i][0]))\n",
        "  print(len(segmented_text[i]))\n",
        "print(first_sentence)\n",
        "1"
      ],
      "metadata": {
        "id": "ImE1guk09M4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentenced_text)"
      ],
      "metadata": {
        "id": "uW9wjmyOhdOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 處理輸入"
      ],
      "metadata": {
        "id": "hK8qwE7U5VSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 將每個段落內的句子合起來變成string\n",
        "podcast_test = [\"\"]*len(segmented_text)\n",
        "for i in range(len(segmented_text)):\n",
        "  for j in range(len(segmented_text[i])):\n",
        "    podcast_test[i]+=segmented_text[i][j]"
      ],
      "metadata": {
        "id": "STBbCVmZ5UzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(podcast_test)"
      ],
      "metadata": {
        "id": "CO9z3rq8hpNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Headline generator"
      ],
      "metadata": {
        "id": "NvjchzbJ4w-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers datasets"
      ],
      "metadata": {
        "id": "3C2ZaL7C47bW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "W43bXEDi5S3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "xpFk5A6q5c9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 使用Michau/t5-base-en-generate-headline"
      ],
      "metadata": {
        "id": "eASYWVCsypFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "headlineGenerator = pipeline(model=\"Michau/t5-base-en-generate-headline\", tokenizer=\"Michau/t5-base-en-generate-headline\")"
      ],
      "metadata": {
        "id": "zw95k5Qh69LA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 生成標題的長度 { display-mode: \"form\" }\n",
        "min_length =  5#@param {type:\"integer\"}\n",
        "max_length = 150#@param {type:\"integer\"}"
      ],
      "metadata": {
        "id": "CIC2gCtlqgiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "headlines = headlineGenerator(podcast_test, min_length, max_length)\n"
      ],
      "metadata": {
        "id": "-NIsP3SV7FQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 顯示結果"
      ],
      "metadata": {
        "id": "1X4gK_W77HYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for headline in headlines:\n",
        "  print(headline)\n",
        "  print(type(headline))"
      ],
      "metadata": {
        "id": "Zqyd9Sp37Jon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "sentence_tokenizer2 = SimpleSentenceTokenizer()\n",
        "index=0\n",
        "headlines_string = [\"\"]*len(headlines)\n",
        "for headline in headlines:\n",
        "  sentenced_healine = re.split(r'([;,\\.\\*\\n-])',headline['generated_text'])\n",
        "  #print(sentenced_healine)\n",
        "  for i in range(len(sentenced_healine)):\n",
        "    sentenced_healine[i] = sentenced_healine[i].strip()\n",
        "    #ss = s1\n",
        "    #print(\"!!!\"+ss+\"!!!\")\n",
        "  print(sentenced_healine)\n",
        "  strs = \" \"  \n",
        "  #headlines_string[index]+=sentenced_healine[0]\n",
        "  for i in range(0,(len(sentenced_healine))):\n",
        "    if(sentenced_healine[i] != \" \"):\n",
        "      strs = sentenced_healine[i]\n",
        "      for j in range(i+1,(len(sentenced_healine))):\n",
        "        if(strs == sentenced_healine[j]):\n",
        "          sentenced_healine[j]=\" \"\n",
        "        #if(sentenced_healine[j] != \" \"):\n",
        "          #strs = sentenced_healine[j]\n",
        "  print(sentenced_healine)\n",
        "  for k in range(len(sentenced_healine)):\n",
        "    if(sentenced_healine[k] != \" \"):\n",
        "      headlines_string[index]+=\" \"\n",
        "      headlines_string[index]+=sentenced_healine[k]\n",
        "  print(type(headlines_string[index]))\n",
        "  headlines_string[index]=headlines_string[index].rstrip(\"-\")\n",
        "  index+=1"
      ],
      "metadata": {
        "id": "ex-Qd6aGN0oH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for headline_string in headlines_string:\n",
        "  print(headline_string)"
      ],
      "metadata": {
        "id": "1hrzGKDrSQoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(headlines_string)\n",
        "1"
      ],
      "metadata": {
        "id": "6fUKcE1-9WDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b = \"abc, ss dd, www\"\n",
        "a = re.split(r\"([,])\",b)\n",
        "print(a)"
      ],
      "metadata": {
        "id": "xfT5GLL8B-RX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 去掉廣告"
      ],
      "metadata": {
        "id": "bJM-zFcqZO5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U scikit-learn\n",
        "!pip install --user -U nltk"
      ],
      "metadata": {
        "id": "4qrAVWgzZUN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer"
      ],
      "metadata": {
        "id": "39F-cSaoZceI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 使用停使用停用詞(NLTK)"
      ],
      "metadata": {
        "id": "dPCYHOs1aKGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 下面的代碼是使用nltk從句子中去除停用詞 \n",
        "# 導入包 \n",
        "import nltk \n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "nltk.download(\"stopwords\")\n",
        "#set(stopwords.words('english')) \n",
        "# 例句 \n",
        "#text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had indeed the vaguest idea where the wood and river in question were.\"\"\" \n",
        "# 停用詞集合 \n",
        "stop_words = set(stopwords.words('english')) \n",
        "# 分詞 \n",
        "nltk.download('punkt')\n",
        "result_nltk = []\n",
        "for sent in podcast_test:\n",
        "  word_tokens = word_tokenize(sent) \n",
        "  filtered_sentence = [] \n",
        "  for w in word_tokens: \n",
        "    if w not in stop_words: \n",
        "      filtered_sentence.append(w) \n",
        "  result_nltk.append(\" \".join(filtered_sentence))"
      ],
      "metadata": {
        "id": "bzS0ux52aNyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(result_nltk))\n",
        "print(result_nltk)"
      ],
      "metadata": {
        "id": "s3EKhQ0Cdrej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer_nltk = CountVectorizer()\n",
        "transformer_nltk = TfidfTransformer()\n",
        "tfidf_nltk = transformer_nltk.fit_transform(vectorizer_nltk.fit_transform(result_nltk))\n",
        "feature_name_nltk = vectorizer_nltk.get_feature_names()"
      ],
      "metadata": {
        "id": "5bTe_ikKdtJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(feature_name_nltk))\n",
        "print(feature_name_nltk)"
      ],
      "metadata": {
        "id": "vdI4EN96d025"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sponsor_n =0\n",
        "for i in range(len(feature_name_nltk)):\n",
        "  if feature_name_nltk[i]=='sponsor':\n",
        "    sponsor_n =i\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "U4gdpbdtd2Ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 找出含sponsor的段落內的keyword和其tfidf\n",
        "from nltk.util import pr\n",
        "print(type(tfidf_nltk))\n",
        "keyword_segment = []\n",
        "tfidf_segment = []\n",
        "#print(tfidf_nltk)\n",
        "#print(tfidf_nltk.toarray())\n",
        "array_tfidf_nltk = tfidf_nltk.toarray()\n",
        "#print(array_tfidf_nltk)\n",
        "#print(array_tfidf_nltk[0])\n",
        "for i in range(len(array_tfidf_nltk)):\n",
        "  if(array_tfidf_nltk[i][sponsor_n]>0):\n",
        "    print(i)\n",
        "    for j in range(len(array_tfidf_nltk[i])):\n",
        "      if(array_tfidf_nltk[i][j]>0):\n",
        "        keyword_segment.append(feature_name_nltk[j])\n",
        "        tfidf_segment.append(array_tfidf_nltk[i][j])\n",
        "        print(feature_name_nltk[j],\" \",array_tfidf_nltk[i][j])"
      ],
      "metadata": {
        "id": "tURlFXlrd4F7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(keyword_segment)\n",
        "print(tfidf_segment)"
      ],
      "metadata": {
        "id": "qp2nIQa_d7eD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 把含sponsor的後lenth句都作為極有可能是sponaor的區塊\n",
        "length = 10\n",
        "sponsor_block = []\n",
        "for i in range(len(sentenced_text)):\n",
        "  ws = word_tokenize(sentenced_text[i])\n",
        "  for w in ws:\n",
        "    if(\"sponsor\" == w or \"sponsors\" == w or \"Sponsor\" == w or \"Sponsors\" == w):\n",
        "      sponsor_block.append(sentenced_text[i])\n",
        "      for h in range(length):\n",
        "        if((i+h)<len(sentenced_text)): sponsor_block.append(sentenced_text[i+h])\n",
        "        \n",
        "      \n",
        "sponsor_block = \" \".join(sponsor_block)\n",
        "print(sponsor_block)"
      ],
      "metadata": {
        "id": "cxazD3cNd731"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 把sponsor block內的keyword和tdidf記下來\n",
        "sponsor_word = word_tokenize(sponsor_block)\n",
        "print(sponsor_word)\n",
        "\n",
        "keyword_sponsor=[]\n",
        "tfidf_sponsor=[]\n",
        "for i in range(len(sponsor_word)):\n",
        "  if(sponsor_word[i] in keyword_segment and sponsor_word[i] not in keyword_sponsor):\n",
        "    keyword_sponsor.append(sponsor_word[i])\n",
        "    tfidf_sponsor.append(tfidf_segment[keyword_segment.index(sponsor_word[i])])\n",
        "print(len(keyword_sponsor))\n",
        "print(len(tfidf_sponsor))\n",
        "n = len(keyword_sponsor)\n",
        "for i in range(n):\n",
        "  for j in range(0,n-i-1):\n",
        "    \n",
        "    if(tfidf_sponsor[j]>tfidf_sponsor[j+1]):\n",
        "      tfidf_sponsor[j], tfidf_sponsor[j+1] =  tfidf_sponsor[j+1], tfidf_sponsor[j]\n",
        "      keyword_sponsor[j], keyword_sponsor[j+1] =  keyword_sponsor[j+1], keyword_sponsor[j]\n",
        "for i in range(0,n):\n",
        "  print(keyword_sponsor[i],\" \",tfidf_sponsor[i])"
      ],
      "metadata": {
        "id": "lXmpUUzLekqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 篩選出名詞和形容詞"
      ],
      "metadata": {
        "id": "iw4YcQmaiSHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "def preprocess(sent):\n",
        "    sent = nltk.pos_tag(sent)\n",
        "    return sent\n",
        "part = preprocess(keyword_sponsor)"
      ],
      "metadata": {
        "id": "L1BV7VcneoLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "select_word = []\n",
        "#part = part.reverse()\n",
        "for i in range(len(part)):\n",
        "  if(part[i][1]=='NN' or part[i][1]=='NNS' or part[i][1]=='JJ'or part[i][1]=='JJR'):\n",
        "    select_word.append(part[i][0])"
      ],
      "metadata": {
        "id": "lpNmMcV-iXRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "select_word"
      ],
      "metadata": {
        "id": "qHcsaaLWiaid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 去掉sponsor"
      ],
      "metadata": {
        "id": "HjftTxNiiqLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentenced_text)):\n",
        "  matches = [a for a in select_word if a in sentenced_text[i]]\n",
        "  #print(all_sentence[i])\n",
        "  print(matches)"
      ],
      "metadata": {
        "id": "140ai3jZipN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num = 5\n",
        "match_num=1\n",
        "before_flag = [0]*num\n",
        "is_sponsor = []\n",
        "for i in range(len(sentenced_text)):\n",
        "  if(\"sponsor\" in sentenced_text[i]):\n",
        "    \n",
        "    for j in range(i,len(sentenced_text)):\n",
        "      matches = [a for a in select_word if a in sentenced_text[j]]\n",
        "      \n",
        "      for f in range(num-1,0,-1):\n",
        "        before_flag[f] = before_flag[f-1]\n",
        "      if(len(matches)>match_num or i==j): before_flag[0] = 1\n",
        "      else: before_flag[0] = 0\n",
        "      print(before_flag)\n",
        "      flag=0\n",
        "      for f in range(num):\n",
        "        if(before_flag[f]==1): \n",
        "          flag=1\n",
        "          break\n",
        "      if(flag==1):\n",
        "        if((j-num-1)>=i): is_sponsor.append(sentenced_text[j-num-1])\n",
        "        \n",
        "      else:\n",
        "        is_sponsor.append(\"----------------------------------------------------------------\")\n",
        "        break\n",
        "    break"
      ],
      "metadata": {
        "id": "z3aYkc0kjOnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sent in is_sponsor: \n",
        "  print(sent)"
      ],
      "metadata": {
        "id": "tjRtFEbhjbMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del_sponsor_sentence = []\n",
        "for i in range(len(sentenced_text)):\n",
        "  if (sentenced_text[i] not in is_sponsor):\n",
        "    del_sponsor_sentence.append(sentenced_text[i])\n",
        "for sent in del_sponsor_sentence: \n",
        "  print(sent)"
      ],
      "metadata": {
        "id": "a9jrZcTPjhfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 重新來分段一次"
      ],
      "metadata": {
        "id": "e54KIYnfj3Mr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strs = \" \"\n",
        "for i in range(len(del_sponsor_sentence)):\n",
        "  if(del_sponsor_sentence[i] != \" \"):\n",
        "    strs =del_sponsor_sentence[i]\n",
        "  if(i+1<len(del_sponsor_sentence)):\n",
        "    if(strs == del_sponsor_sentence[i+1]):\n",
        "      del_sponsor_sentence[i+1]=\" \"\n",
        "vecr = CountVectorizer(vocabulary=wrdvecs.index)\n",
        "\n",
        "sentence_vectors = vecr.transform(del_sponsor_sentence).dot(wrdvecs)\n",
        "new_length = len(del_sponsor_sentence)**0.6\n",
        "print(new_length)\n",
        "penalty = get_penalty([sentence_vectors], new_length)\n",
        "print('penalty %4.2f' % penalty)\n",
        "\n",
        "optimal_segmentation = split_optimal(sentence_vectors, penalty, seg_limit=250)\n",
        "segmented_text = get_segments(del_sponsor_sentence, optimal_segmentation)\n",
        "\n",
        "print('%d sentences, %d segments, avg %4.2f sentences per segment' % (\n",
        "    len(del_sponsor_sentence), len(segmented_text), len(del_sponsor_sentence) / len(segmented_text)))\n",
        "\n",
        "with open(book_path + '.seg', 'wt') as f:\n",
        "    for i, segment_sentences in enumerate(segmented_text):\n",
        "        segment_str = ' // '.join(segment_sentences)\n",
        "        gain = optimal_segmentation.gains[i] if i < len(segmented_text) - 1 else 0\n",
        "        segment_info = ' [%d sentences, %4.3f] ' % (len(segment_sentences), gain) \n",
        "        print(segment_str + '\\n8<' + '=' * 30 + segment_info + \"=\" * 30, file=f)\n",
        "\n",
        "greedy_segmentation = split_greedy(sentence_vectors, max_splits=len(optimal_segmentation.splits))\n",
        "greedy_segmented_text = get_segments(del_sponsor_sentence, greedy_segmentation)\n",
        "lengths_optimal = [len(segment) for segment in segmented_text for sentence in segment]\n",
        "lengths_greedy = [len(segment) for segment in greedy_segmented_text for sentence in segment]\n",
        "df = pd.DataFrame({'greedy':lengths_greedy, 'optimal': lengths_optimal})\n",
        "df.plot.line(figsize=(18, 3), title='Segment lenghts over text')\n",
        "df.plot.hist(bins=30, alpha=0.5, figsize=(10, 3), title='Histogram of segment lengths')\n",
        "\n",
        "totals = [get_total(sentence_vectors, seg.splits, penalty) \n",
        "          for seg in [optimal_segmentation, greedy_segmentation]]\n",
        "print('optimal score %4.2f, greedy score %4.2f' % tuple(totals))\n",
        "print(totals)\n",
        "print(tuple(totals))\n",
        "print('ratio of scores %5.4f' % (totals[0] / totals[1]))"
      ],
      "metadata": {
        "id": "boaB1_ULj7Xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(segmented_text)):\n",
        "  print(segmented_text[i])"
      ],
      "metadata": {
        "id": "7sTKAqBAkNs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 將每個段落內的句子合起來變成string\n",
        "podcast_test = [\"\"]*len(segmented_text)\n",
        "for i in range(len(segmented_text)):\n",
        "  for j in range(len(segmented_text[i])):\n",
        "    podcast_test[i]+=segmented_text[i][j]"
      ],
      "metadata": {
        "id": "t2WSn6cikSTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "headlines = headlineGenerator(podcast_test, min_length, max_length)"
      ],
      "metadata": {
        "id": "7QVXj81ikiWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for headline in headlines:\n",
        "  print(headline)\n",
        "  "
      ],
      "metadata": {
        "id": "-rdoPJtkko4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "sentence_tokenizer2 = SimpleSentenceTokenizer()\n",
        "index=0\n",
        "headlines_string = [\"\"]*len(headlines)\n",
        "for headline in headlines:\n",
        "  sentenced_healine = re.split(r'([;,\\.\\*\\n-])',headline['generated_text'])\n",
        "  #print(sentenced_healine)\n",
        "  for i in range(len(sentenced_healine)):\n",
        "    sentenced_healine[i] = sentenced_healine[i].strip()\n",
        "    #ss = s1\n",
        "    #print(\"!!!\"+ss+\"!!!\")\n",
        "  print(sentenced_healine)\n",
        "  strs = \" \"  \n",
        "  #headlines_string[index]+=sentenced_healine[0]\n",
        "  for i in range(0,(len(sentenced_healine))):\n",
        "    if(sentenced_healine[i] != \" \"):\n",
        "      strs = sentenced_healine[i]\n",
        "      for j in range(i+1,(len(sentenced_healine))):\n",
        "        if(strs == sentenced_healine[j]):\n",
        "          sentenced_healine[j]=\" \"\n",
        "        #if(sentenced_healine[j] != \" \"):\n",
        "          #strs = sentenced_healine[j]\n",
        "  print(sentenced_healine)\n",
        "  for k in range(len(sentenced_healine)):\n",
        "    if(sentenced_healine[k] != \" \"):\n",
        "      headlines_string[index]+=\" \"\n",
        "      headlines_string[index]+=sentenced_healine[k]\n",
        "  print(type(headlines_string[index]))\n",
        "  headlines_string[index]=headlines_string[index].rstrip(\"-\")\n",
        "  index+=1"
      ],
      "metadata": {
        "id": "8XhAZ-2ek5CL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for headline_string in headlines_string:\n",
        "  print(headline_string)"
      ],
      "metadata": {
        "id": "nRSHPWdMk79T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(headlines_string)"
      ],
      "metadata": {
        "id": "Kct_33gSqscV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(headlines_string)\n",
        "3"
      ],
      "metadata": {
        "id": "GdARSdfLSSLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_sentence = []\n",
        "for i in range(len(segmented_text)):\n",
        "  first_sentence.append(segmented_text[i][0].strip().rstrip().rstrip('.').rstrip('?'))\n",
        "  print(segmented_text[i][0])\n",
        "  print(type(segmented_text[i][0]))\n",
        "  print(len(segmented_text[i]))"
      ],
      "metadata": {
        "id": "ESo9qnjESTuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(first_sentence)\n",
        "print(len((first_sentence)))"
      ],
      "metadata": {
        "id": "Tl4aWOUiSXQn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}